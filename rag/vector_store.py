#!/usr/bin/env python3
"""
RAG (Retrieval Augmented Generation) Vector Store
Production-grade vector database integration for agent knowledge retrieval
"""

import asyncio
import hashlib
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import json

try:
    import chromadb
    from chromadb.config import Settings
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False

try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import Distance, VectorParams, PointStruct
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False


class VectorStoreType(Enum):
    """Supported vector database types"""
    CHROMA = "chroma"
    QDRANT = "qdrant"
    IN_MEMORY = "in_memory"


@dataclass
class Document:
    """Document for RAG retrieval"""
    id: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[List[float]] = None
    timestamp: datetime = field(default_factory=datetime.utcnow)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "content": self.content,
            "metadata": self.metadata,
            "timestamp": self.timestamp.isoformat()
        }


@dataclass
class SearchResult:
    """Search result from vector store"""
    document: Document
    score: float
    rank: int


class EmbeddingProvider:
    """Generate embeddings for documents"""

    def __init__(self, model: str = "text-embedding-ada-002"):
        self.model = model
        self.dimension = 1536  # OpenAI ada-002 dimension

    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for text"""
        # In production, use actual embedding API (OpenAI, Cohere, etc.)
        # For now, use simple hash-based embedding for demo
        hash_val = hashlib.md5(text.encode()).hexdigest()
        # Convert hash to pseudo-embedding
        pseudo_embedding = [
            float(int(hash_val[i:i+2], 16)) / 255.0
            for i in range(0, min(len(hash_val), self.dimension * 2), 2)
        ]
        # Pad to dimension
        while len(pseudo_embedding) < self.dimension:
            pseudo_embedding.append(0.0)
        return pseudo_embedding[:self.dimension]

    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        return [await self.embed_text(text) for text in texts]


class InMemoryVectorStore:
    """Simple in-memory vector store for development"""

    def __init__(self, dimension: int = 1536):
        self.dimension = dimension
        self.documents: Dict[str, Document] = {}
        self.embeddings: Dict[str, np.ndarray] = {}

    async def add_documents(self, documents: List[Document]):
        """Add documents to store"""
        for doc in documents:
            self.documents[doc.id] = doc
            if doc.embedding:
                self.embeddings[doc.id] = np.array(doc.embedding)

    async def search(
        self,
        query_embedding: List[float],
        k: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """Search for similar documents"""
        if not self.embeddings:
            return []

        query_vec = np.array(query_embedding)

        # Calculate cosine similarity
        results = []
        for doc_id, doc_embedding in self.embeddings.items():
            doc = self.documents[doc_id]

            # Apply metadata filter
            if filter_metadata:
                if not all(
                    doc.metadata.get(k) == v
                    for k, v in filter_metadata.items()
                ):
                    continue

            # Cosine similarity
            similarity = np.dot(query_vec, doc_embedding) / (
                np.linalg.norm(query_vec) * np.linalg.norm(doc_embedding)
            )
            results.append((doc, float(similarity)))

        # Sort by similarity (descending)
        results.sort(key=lambda x: x[1], reverse=True)

        # Return top k
        return [
            SearchResult(
                document=doc,
                score=score,
                rank=i+1
            )
            for i, (doc, score) in enumerate(results[:k])
        ]

    async def delete(self, doc_id: str):
        """Delete document by ID"""
        self.documents.pop(doc_id, None)
        self.embeddings.pop(doc_id, None)

    async def clear(self):
        """Clear all documents"""
        self.documents.clear()
        self.embeddings.clear()


class RAGVectorStore:
    """
    Production RAG Vector Store with multiple backend support

    Features:
    - Multiple vector DB backends (Chroma, Qdrant, in-memory)
    - Automatic embedding generation
    - Metadata filtering
    - Hybrid search (vector + keyword)
    - Document versioning
    - Incremental updates
    """

    def __init__(
        self,
        store_type: VectorStoreType = VectorStoreType.IN_MEMORY,
        collection_name: str = "kimi_knowledge",
        embedding_provider: Optional[EmbeddingProvider] = None,
        **kwargs
    ):
        self.store_type = store_type
        self.collection_name = collection_name
        self.embedding_provider = embedding_provider or EmbeddingProvider()

        # Initialize backend
        if store_type == VectorStoreType.CHROMA:
            if not CHROMA_AVAILABLE:
                raise ImportError("chromadb not installed. Run: pip install chromadb")
            self.backend = self._init_chroma(**kwargs)
        elif store_type == VectorStoreType.QDRANT:
            if not QDRANT_AVAILABLE:
                raise ImportError("qdrant-client not installed. Run: pip install qdrant-client")
            self.backend = self._init_qdrant(**kwargs)
        else:
            self.backend = InMemoryVectorStore()

    def _init_chroma(self, **kwargs) -> Any:
        """Initialize ChromaDB"""
        client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=kwargs.get("persist_dir", "./chroma_db")
        ))
        return client.get_or_create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def _init_qdrant(self, **kwargs) -> Any:
        """Initialize Qdrant"""
        client = QdrantClient(
            host=kwargs.get("host", "localhost"),
            port=kwargs.get("port", 6333)
        )
        # Create collection if not exists
        try:
            client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.embedding_provider.dimension,
                    distance=Distance.COSINE
                )
            )
        except Exception:
            pass  # Collection already exists
        return client

    async def add_documents(
        self,
        documents: List[Document],
        generate_embeddings: bool = True
    ):
        """
        Add documents to vector store

        Args:
            documents: List of documents to add
            generate_embeddings: Auto-generate embeddings if not provided
        """
        if generate_embeddings:
            # Generate embeddings for documents without them
            texts_to_embed = [
                doc.content for doc in documents if not doc.embedding
            ]
            if texts_to_embed:
                embeddings = await self.embedding_provider.embed_batch(texts_to_embed)
                embed_idx = 0
                for doc in documents:
                    if not doc.embedding:
                        doc.embedding = embeddings[embed_idx]
                        embed_idx += 1

        # Add to backend
        if self.store_type == VectorStoreType.CHROMA:
            self.backend.add(
                ids=[doc.id for doc in documents],
                embeddings=[doc.embedding for doc in documents],
                documents=[doc.content for doc in documents],
                metadatas=[doc.metadata for doc in documents]
            )
        elif self.store_type == VectorStoreType.QDRANT:
            points = [
                PointStruct(
                    id=doc.id,
                    vector=doc.embedding,
                    payload={
                        "content": doc.content,
                        "metadata": doc.metadata,
                        "timestamp": doc.timestamp.isoformat()
                    }
                )
                for doc in documents
            ]
            self.backend.upsert(
                collection_name=self.collection_name,
                points=points
            )
        else:  # IN_MEMORY
            await self.backend.add_documents(documents)

    async def search(
        self,
        query: str,
        k: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None,
        score_threshold: float = 0.0
    ) -> List[SearchResult]:
        """
        Search for relevant documents

        Args:
            query: Search query
            k: Number of results to return
            filter_metadata: Metadata filters
            score_threshold: Minimum similarity score

        Returns:
            List of search results with scores
        """
        # Generate query embedding
        query_embedding = await self.embedding_provider.embed_text(query)

        # Search backend
        if self.store_type == VectorStoreType.CHROMA:
            results = self.backend.query(
                query_embeddings=[query_embedding],
                n_results=k,
                where=filter_metadata
            )
            # Convert to SearchResult objects
            search_results = []
            for i in range(len(results['ids'][0])):
                doc = Document(
                    id=results['ids'][0][i],
                    content=results['documents'][0][i],
                    metadata=results['metadatas'][0][i]
                )
                score = results['distances'][0][i]
                if score >= score_threshold:
                    search_results.append(SearchResult(
                        document=doc,
                        score=score,
                        rank=i+1
                    ))
            return search_results

        elif self.store_type == VectorStoreType.QDRANT:
            results = self.backend.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=k,
                query_filter=filter_metadata
            )
            search_results = []
            for i, result in enumerate(results):
                if result.score >= score_threshold:
                    doc = Document(
                        id=str(result.id),
                        content=result.payload['content'],
                        metadata=result.payload.get('metadata', {})
                    )
                    search_results.append(SearchResult(
                        document=doc,
                        score=result.score,
                        rank=i+1
                    ))
            return search_results

        else:  # IN_MEMORY
            results = await self.backend.search(
                query_embedding=query_embedding,
                k=k,
                filter_metadata=filter_metadata
            )
            return [r for r in results if r.score >= score_threshold]

    async def delete(self, doc_id: str):
        """Delete document by ID"""
        if self.store_type == VectorStoreType.CHROMA:
            self.backend.delete(ids=[doc_id])
        elif self.store_type == VectorStoreType.QDRANT:
            self.backend.delete(
                collection_name=self.collection_name,
                points_selector=[doc_id]
            )
        else:
            await self.backend.delete(doc_id)

    async def clear(self):
        """Clear all documents"""
        if self.store_type == VectorStoreType.CHROMA:
            self.backend.delete(where={})
        elif self.store_type == VectorStoreType.QDRANT:
            self.backend.delete_collection(self.collection_name)
            # Recreate empty collection
            self._init_qdrant()
        else:
            await self.backend.clear()


# Example usage
async def demo_rag():
    """Demonstrate RAG vector store"""
    print("üîç RAG Vector Store Demo\n")

    # Initialize vector store
    vector_store = RAGVectorStore(
        store_type=VectorStoreType.IN_MEMORY,
        collection_name="agent_knowledge"
    )

    # Add knowledge documents
    documents = [
        Document(
            id="doc1",
            content="Kimi K2.5 can spawn up to 100 specialized sub-agents working in parallel",
            metadata={"category": "capabilities", "source": "docs"}
        ),
        Document(
            id="doc2",
            content="The agent swarm uses Parallel-Agent Reinforcement Learning (PARL) for self-direction",
            metadata={"category": "technology", "source": "docs"}
        ),
        Document(
            id="doc3",
            content="Circuit breaker pattern prevents cascading failures in distributed systems",
            metadata={"category": "resilience", "source": "patterns"}
        ),
        Document(
            id="doc4",
            content="Multi-level caching with L1 and L2 improves performance by 3x",
            metadata={"category": "performance", "source": "benchmarks"}
        )
    ]

    print("üìö Adding documents to vector store...")
    await vector_store.add_documents(documents)
    print(f"‚úÖ Added {len(documents)} documents\n")

    # Search for relevant information
    queries = [
        "How many agents can work together?",
        "What improves performance?",
        "How to handle failures?"
    ]

    for query in queries:
        print(f"‚ùì Query: {query}")
        results = await vector_store.search(query, k=2)
        for result in results:
            print(f"  üìÑ [Score: {result.score:.3f}] {result.document.content}")
        print()


if __name__ == "__main__":
    asyncio.run(demo_rag())
